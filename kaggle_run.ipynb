{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-21T16:26:52.842976Z","iopub.execute_input":"2024-07-21T16:26:52.843294Z","iopub.status.idle":"2024-07-21T16:26:53.337236Z","shell.execute_reply.started":"2024-07-21T16:26:52.843267Z","shell.execute_reply":"2024-07-21T16:26:53.336375Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"PREFIX_INPUT = \"/kaggle/input/playground-series-s4e7\"\nPREFIX_OUTPUT = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:26:53.343196Z","iopub.execute_input":"2024-07-21T16:26:53.343596Z","iopub.status.idle":"2024-07-21T16:26:53.348883Z","shell.execute_reply.started":"2024-07-21T16:26:53.343559Z","shell.execute_reply":"2024-07-21T16:26:53.347701Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(f'{PREFIX_INPUT}/train.csv')\ntest = pd.read_csv(f'{PREFIX_INPUT}/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-21T16:26:53.350085Z","iopub.execute_input":"2024-07-21T16:26:53.350426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:25:07.353727Z","iopub.execute_input":"2024-07-21T07:25:07.354217Z","iopub.status.idle":"2024-07-21T07:25:07.360241Z","shell.execute_reply.started":"2024-07-21T07:25:07.354181Z","shell.execute_reply":"2024-07-21T07:25:07.358658Z"}}},{"cell_type":"code","source":"# train[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: \n+ no null\n+ label: Response \n+ id: no use\n+ Gender: binary \n+ Age: -> change to category\n+ Driving_License: binary  \n+ Region_Code: change to category\n+ previous_insured: binary \n+ Vehicle_Damage: binary\n+ Vehicle_Damage: category\n+ Annual_Premium: numerical \n+ Policy_Sales_Channel: -> change to category\n+ vintage: -> change to category\n\nImportant: \n+ this is imbalance dataset","metadata":{}},{"cell_type":"markdown","source":"## EDA & Feature Selection","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:25:34.226328Z","iopub.execute_input":"2024-07-21T07:25:34.226717Z","iopub.status.idle":"2024-07-21T07:25:34.231964Z","shell.execute_reply.started":"2024-07-21T07:25:34.226688Z","shell.execute_reply":"2024-07-21T07:25:34.230483Z"}}},{"cell_type":"markdown","source":"### Binary cols","metadata":{}},{"cell_type":"code","source":"import seaborn as sns \nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby('Previously_Insured')[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby('Gender')[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby('Driving_License')[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby('Vehicle_Age')[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.groupby('Vehicle_Damage')[\"Response\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: \n+ people with no insure has higher chance to respond\n+ Male has slightly higher rate of response\n+ people without license is not likely to respond\n+ the more people ride, the more they respond\n+ people with vihicle damage arre likely to respond","metadata":{"execution":{"iopub.status.busy":"2024-07-21T07:27:08.878932Z","iopub.execute_input":"2024-07-21T07:27:08.880352Z","iopub.status.idle":"2024-07-21T07:27:08.888358Z","shell.execute_reply.started":"2024-07-21T07:27:08.880301Z","shell.execute_reply":"2024-07-21T07:27:08.886444Z"}}},{"cell_type":"markdown","source":"### Numerical cols","metadata":{}},{"cell_type":"markdown","source":"Age, Region_Code, Annual_Premium, Policy_Sales_Channel, Vintage\n","metadata":{}},{"cell_type":"code","source":"def plot_all_cols(data,cols):\n    id_res = (data[\"Response\"] == 1)\n    id_no_res = (data[\"Response\"] == 0)\n    l = len(cols)\n    plt.figure(figsize=(20,5*l))\n    for i,col in enumerate(cols):\n        print(col)\n        plt.subplot(l,2,i*2+1)\n        value,count = np.unique(data[col].loc[id_res],return_counts=True)\n        plt.bar(value,count)\n        plt.subplot(l,2,i*2+2)\n        value,count = np.unique(data[col].loc[id_no_res],return_counts=True)\n        plt.bar(value,count,color='r')\n    plt.tight_layout()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot_all_cols(train,[\"Age\",\"Region_Code\",\"Policy_Sales_Channel\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cal_pct_diff(data,col):\n    id_res = (data[\"Response\"] == 1)\n    id_no_res = (data[\"Response\"] == 0)\n    value_res,count_res = np.unique(data[col].loc[id_res],return_counts=True)\n    value_no_res,count_no_res = np.unique(data[col].loc[id_no_res],return_counts=True)\n    res = {}\n    for i in range(len(value_res)):\n        res[value_res[i]] = count_res[i]/count_no_res[i]\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pct_res = (train[\"Response\"] == 1).sum()/(train[\"Response\"] == 0).sum()\npct_res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_pct = cal_pct_diff(train,\"Vintage\")\nkeys = np.array(list(dict_pct.keys()))\nvalues = np.array(list(dict_pct.values()))\n# plt.figure(figsize=(20,5))\n# plt.bar(keys,values)\n# plt.plot([10,300],[pct_res,pct_res],'r')\n# plt.plot([10,300],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_plot_range_pct(range_pct,keys,values,pct_res=pct_res):\n    plt.bar(keys,sorted(values))\n    plt.plot([min(keys),max(keys)],[pct_res,pct_res],'r')\n    for pct in range_pct:\n        plt.plot([min(keys),max(keys)],[pct,pct],'g')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct = [0.08,0.1,0.12,0.17,0.22,0.28,0.3,0.33,0.37]\n# draw_plot_range_pct(range_pct,keys,values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_groups(df, column_name,keys,values, range_pct):    \n    bins = [float('-inf')] + range_pct + [float('inf')]\n    cutten_groups = []\n    \n    for i in range(1, len(bins)):\n        idx = (values > bins[i-1]) & (values <= bins[i])\n        cutten_groups.append(keys[idx])\n    \n    # Check if any key is missing\n    total_grouped = sum(len(g) for g in cutten_groups)\n    if total_grouped != len(keys):\n        print(f\"Warning: {len(keys) - total_grouped} keys are missing from groups\")\n    \n    dict_cutten_groups = {}\n    for idx, group in enumerate(cutten_groups):\n        for value in group:\n            dict_cutten_groups[value] = f'Group{idx}'\n    \n    new_column_name = f'{column_name}Group'\n    df[new_column_name] = df[column_name].map(dict_cutten_groups).fillna('Unknown')\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_groups(train,\"Vintage\",keys,values,range_pct)\ncreate_groups(test,\"Vintage\",keys,values,range_pct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"VintageGroup\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_pct = cal_pct_diff(train,\"Region_Code\")\nkeys = np.array(list(dict_pct.keys()))\nvalues = np.array(list(dict_pct.values()))\n# idx = values < 0.9\n# plt.bar(keys[idx],values[idx])\n# plt.bar(keys[~idx],1,color='r')\n# plt.plot([0,52],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# values_tmp = values.copy() \n# values_tmp[~idx] = 0.9","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct = [0.04,0.07,0.11,0.2,0.26,0.42,0.6,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw_plot_range_pct(range_pct=range_pct,keys=keys,values=values_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct += [3] # add for the region code 39 and 40","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_groups(train,\"Region_Code\",keys,values,range_pct)\ncreate_groups(test,\"Region_Code\",keys,values,range_pct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"Policy_Sales_Channel\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_pct = cal_pct_diff(train,\"Policy_Sales_Channel\")\nkeys = np.array(list(dict_pct.keys()))\nvalues = np.log1p(np.array(list(dict_pct.values())))\n# idx = values < 0.4\n# # plt.bar(keys,values)\n# plt.bar(keys[idx],values[idx])\n# # plt.bar(keys[~idx],1,color='r')\n# plt.plot([min(keys),max(keys)],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# values_tmp = values.copy() \n# values_tmp[~idx] = 0.4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct = [0.05,0.1,0.2,0.25,0.39]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw_plot_range_pct(range_pct=range_pct,keys=keys,values=values_tmp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx2 = values > 0.4\n# # plt.bar(keys,values)\n# plt.bar(keys[idx2],values[idx2])\n# # plt.bar(keys[~idx],1,color='r')\n# plt.plot([min(keys),max(keys)],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct_2 = [1.1,2,2.7,3.3,4,4.9,6.8]\n# draw_plot_range_pct(range_pct=range_pct_2,keys=keys,values=values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_range_pct = range_pct + range_pct_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_groups(train,\"Policy_Sales_Channel\",keys,values,final_range_pct)\ncreate_groups(test,\"Policy_Sales_Channel\",keys,values,range_pct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"Policy_Sales_ChannelGroup\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_pct = cal_pct_diff(train,\"Age\")\nkeys = np.array(list(dict_pct.keys()))\nvalues = np.array(list(dict_pct.values()))\n# plt.figure(figsize=(20,5))\n# plt.bar(keys,values)\n# plt.plot([20,84],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_pct = [0.04,0.07,0.12,0.16,0.205,0.235,0.265,0.287]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw_plot_range_pct(range_pct=range_pct,keys=keys,values=values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_groups(train,\"Age\",keys,values,range_pct)\ncreate_groups(test,\"Age\",keys,values,range_pct)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"AgeGroup\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"Annual_log\"] = train[\"Annual_Premium\"].apply(np.log1p)\ntest[\"Annual_log\"] = test[\"Annual_Premium\"].apply(np.log1p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"Annual_log\"].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins = [float('-inf'), 8.0, 8.5,9.0,9.5,10,10.5,11,11.5,12,12.5,13,float('inf')]\nlabels = ['-8','8-8.5','8.5-9','9-9.5','9.5-10','10-10.5','10.5-11','11-11.5','11.5-12','12-12.5','12.5-13','13+']\ntrain.loc[:, \"AnnualGroup\"]= pd.cut(train[\"Annual_log\"],bins=bins,labels=labels,right=True, include_lowest=True)\ntest.loc[:, \"AnnualGroup\"]= pd.cut(test[\"Annual_log\"],bins=bins,labels=labels,right=True, include_lowest=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped_age= train.groupby(['AnnualGroup', 'Response']).size().unstack(fill_value=0)\n# grouped_age.plot(kind='bar', stacked=False, color=['red', 'green'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del grouped_age","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_pct = cal_pct_diff(train,\"AnnualGroup\")\nkeys = np.array(list(dict_pct.keys()))\nvalues = np.array(list(dict_pct.values()))\n# plt.figure(figsize=(20,5))\n# plt.bar(keys,values)\n# plt.plot([0,len(keys)-1],[pct_res,pct_res],'r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del dict_pct\ndel keys\ndel values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train[\"AnnualGroup\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"code","source":"columns = [\"Gender\",\"Driving_License\",\"Previously_Insured\",\"Vehicle_Age\",\"Vehicle_Damage\",\"Region_CodeGroup\",\"Policy_Sales_ChannelGroup\",\"VintageGroup\",\"AgeGroup\",\"AnnualGroup\",\"Annual_Premium\"]\nlabel = [\"Response\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = train[columns+label]\nX_train = df_train.drop(columns=label)\ny_train = df_train[label]\nX_test = test[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_cols = [\"Gender\",\"Driving_License\",\"Previously_Insured\",\"Vehicle_Damage\"]\nmulti_cols = [\"Vehicle_Age\",\"Region_CodeGroup\",\"Policy_Sales_ChannelGroup\",\"VintageGroup\",\"AgeGroup\",\"AnnualGroup\"]\nnum_cols = [\"Annual_Premium\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One hot encoding\nX_train = pd.get_dummies(X_train, columns=multi_cols)\nX_train = pd.get_dummies(X_train, columns=binary_cols, drop_first=True)\nX_test = pd.get_dummies(X_test, columns=multi_cols)\nX_test = pd.get_dummies(X_test, columns=binary_cols, drop_first=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardization\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.fit_transform(X_test[num_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat_df_train = pd.concat([X_train,y_train],axis=1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# corr_matrix = concat_df_train.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# corr_threshold  = 0.1\n# high_corr_cols = corr_matrix[abs(corr_matrix['Response']) > corr_threshold].index\n# sorted_corr_matrix = concat_df_train[high_corr_cols].corr()\n# sorted_corr_matrix['Response'].sort_values(ascending=False)\n# sns.heatmap(sorted_corr_matrix, annot=True,cmap='coolwarm')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del corr_matrix\n# del sorted_corr_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choosing Model","metadata":{}},{"cell_type":"markdown","source":"### Split val-train","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42,stratify=y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts(), y_val.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RandomForest","metadata":{}},{"cell_type":"markdown","source":"Because the dataset is too large, so I will use Chunking technique.","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import roc_auc_score, f1_score\n\n# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, warm_start=True)\n# chunk_size = 100000  # Adjust based on your available memory\n# for i in range(0, len(X_train), chunk_size):\n#     X_chunk = X_train[i:i+chunk_size]\n#     y_chunk = y_train[i:i+chunk_size].values.reshape(-1)\n#     rf_classifier.n_estimators += 10  # Grow 10 new trees each iteration\n#     rf_classifier.fit(X_chunk, y_chunk)\n\n# y_pred = rf_classifier.predict(X_val)\n\n# # Evaluate the model\n# auc = roc_auc_score(y_val, y_pred)\n# f1 = f1_score(y_val, y_pred)\n# print(f'AUC: {auc:.4f}')\n# print(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import roc_auc_score, f1_score\n\n# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, warm_start=True)\n# rf_classifier.fit(X_train, y_train.values.reshape(-1))\n# y_pred = rf_classifier.predict(X_val)\n\n# # Evaluate the model\n# auc = roc_auc_score(y_val, y_pred)\n# f1 = f1_score(y_val, y_pred)\n# print(f'AUC: {auc:.4f}')\n# print(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import joblib\n# joblib.dump(rf_classifier, f'{PREFIX_OUTPUT}/rf_classifier_model.joblib')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Using partial_fit of SGD","metadata":{}},{"cell_type":"code","source":"# from sklearn.linear_model import SGDClassifier\n# from sklearn.metrics import roc_auc_score, f1_score\n# clf = SGDClassifier(loss='log_loss', random_state=42)\n\n# batch_size = 1000000\n# for i in range(0, X_train.shape[0], batch_size):\n#     print(f'Training batch {i // batch_size + 1}/{X_train.shape[0] // batch_size}')\n#     X_batch = X_train[i:i + batch_size]\n#     y_batch = y_train[i:i + batch_size].values.reshape(-1)\n#     clf.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n\n# # Predict on the validation set\n# y_pred = clf.predict(X_val)\n\n# # Evaluate the model\n# auc = roc_auc_score(y_val, y_pred)\n# f1 = f1_score(y_val, y_pred)\n# print(f'AUC: {auc:.4f}')\n# print(f'F1 Score: {f1:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change name of Vehicle_Age_<1 year and Vehicle_Age_> 2 Years\nX_train = X_train.rename(columns={\n    'Vehicle_Age_< 1 Year': 'Vehicle_Age_LessThanOneYear',\n    'Vehicle_Age_> 2 Years': 'Vehicle_Age_MoreThanTwoYear',\n    'Vehicle_Age_1-2 Year': 'Vehicle_Age_1-TwoYear'\n})\nX_val = X_val.rename(columns={\n    'Vehicle_Age_< 1 Year': 'Vehicle_Age_LessThanOneYear',\n    'Vehicle_Age_> 2 Years': 'Vehicle_Age_MoreThanTwoYear',\n    'Vehicle_Age_1-2 Year': 'Vehicle_Age_1-TwoYear'\n})\nX_test = X_test.rename(columns={\n    'Vehicle_Age_< 1 Year': 'Vehicle_Age_LessThanOneYear',\n    'Vehicle_Age_> 2 Years': 'Vehicle_Age_MoreThanTwoYear',\n    'Vehicle_Age_1-2 Year': 'Vehicle_Age_1-TwoYear'\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cols = X_test.columns\ntrain_cols = X_train.columns\nset1 = set(test_cols.tolist())\nset2 = set(train_cols.tolist())\nmissing_cols = list(set2.difference(set1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_cols),len(train_cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in missing_cols: \n    X_test[col] = False ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_test.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test[sorted(X_test.columns)]\nX_train = X_train[sorted(X_train.columns)]\nX_val = X_val[sorted(X_val.columns)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtest = xgb.DMatrix(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aucs = []\npreds = []\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train)):\n    print(f'### Fold {fold+1} Training ###')\n    X_train_fold = X_train.iloc[train_idx]\n    y_train_fold = y_train.iloc[train_idx]\n    X_val_fold = X_train.iloc[valid_idx]\n    y_val_fold = y_train.iloc[valid_idx]\n    \n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'random_state': 42,\n        'tree_method': 'hist',  # Use GPU acceleration\n        'device': \"cuda\"  # Use the first GPU. Change if you want to use a different GPU\n    }\n    print(\"Start changing type to DMatrix...\")\n    dtrain = xgb.DMatrix(X_train_fold, label=y_train_fold)\n    del X_train_fold\n    del y_train_fold\n    dval = xgb.DMatrix(X_val_fold)\n    del X_val_fold\n    \n    print(\"Start training...\")\n    bst = xgb.train(params, dtrain, num_boost_round=100, evals=[(dval, 'eval')], early_stopping_rounds=50, verbose_eval=100)\n    \n    print(\"Start predicting val...\")\n    # Evaluate the model\n    y_pred_proba = bst.predict(dval)\n    auc_score = roc_auc_score(y_val_fold, y_pred_proba)\n    del y_pred_proba\n    del y_val_fold\n    aucs.append(auc_score)\n    print(f'Fold {fold+1} AUC: {auc_score:.5f}\\n')\n    \n    # Pred test set\n    print(\"Start predicting test...\")\n    dtest = xgb.DMatrix(X_test)  # Assuming you have X_test defined\n    y_test_pred_proba = bst.predict(dtest)\n    submission = test[['id']]\n    submission['Response'] = y_test_pred_proba\n    submission.to_csv(f'submission_{Fold}.csv', index=False)\n    del submission\n    del y_test_pred_proba\n    \n    \n\nprint(f'\\nOverall AUC: {np.mean(aucs):.5f} +/- {np.std(aucs):.5f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}